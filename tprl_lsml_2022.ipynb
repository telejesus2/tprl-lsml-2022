{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh7i2Ubs2VhP"
      },
      "source": [
        "# Install dependencies (run only once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q99baqG2suip"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/telejesus2/tprl-lsml-2022.git\n",
        "%load tprl-lsml-2022/agents/common_all.py\n",
        "import sys\n",
        "sys.path.insert(0,'/content/tprl-lsml-2022')\n",
        "!pip install pybullet\n",
        "!pip install gym==0.18.0\n",
        "!pip install box2d-py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxontUyxqQUt"
      },
      "outputs": [],
      "source": [
        "!apt-get install swig cmake libopenmpi-dev zlib1g-dev xvfb x11-utils ffmpeg -qq \n",
        "\n",
        "from natsort import natsorted\n",
        "from pathlib import Path\n",
        "import base64\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Set up fake display; otherwise rendering will fail\n",
        "import os\n",
        "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
        "os.environ['DISPLAY'] = ':1'\n",
        "\n",
        "# Display video\n",
        "def show_videos(video_path='', prefix=''):\n",
        "  html = []\n",
        "  for mp4 in natsorted(Path(video_path).glob(\"{}*.mp4\".format(prefix))):\n",
        "      video_b64 = base64.b64encode(mp4.read_bytes())\n",
        "      html.append('''<video alt=\"{}\" autoplay \n",
        "                    loop controls style=\"height: 400px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(mp4, video_b64.decode('ascii')))\n",
        "  ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjZ12QpGvtTj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "\n",
        "from utils.utils import PiecewiseSchedule\n",
        "from environments.gym import GymEnv\n",
        "from networks.policy import Policy\n",
        "from networks.value_fun import ValueFunction\n",
        "from networks.q_fun import QFunction\n",
        "from agents.common_on_policy import OnPolicyAgent\n",
        "from agents.common_off_policy import OffPolicyAgent\n",
        "from utils.plot import plot\n",
        "import utils.logz as logz\n",
        "\n",
        "# define device\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cBDTCofmzJFx"
      },
      "outputs": [],
      "source": [
        "cd tprl-lsml-2022/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VVi9x3DuKHQ"
      },
      "source": [
        "# 1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLg-1J6IuPKz"
      },
      "source": [
        "*   Do not forget to enable the GPU.\n",
        "*   We recommend SAVING THE RESULTS of your experiments as you go (download to your computer the log.txt and params.json files inside the EXPERIMENTS folder).\n",
        "*   Each training might take a LONG TIME if you are unlucky (up to 25 minutes). We recommend to open a separate copy of the notebook to debug your code for the final questions while you run experiments here.\n",
        "*   Try not to stop a cell that is currently training, as it might mess up the rendering of the agent in future runs. If you encounter such a problem, you might have to reset the notebook. Before doing so, do not forget to download to your computer the log.txt and params.json files of your experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9E0ouvZ_9uh"
      },
      "source": [
        "YOUR CODE STARTS HERE appears wherever you are expected to write your own code (questions 3.2 and 4.1). For the experiments, there are some recommended hyper-parameters as comments in the code, but they might not be optimal so feel free to change them. We ask you to plot your results with the plot function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcXttTvq-bBx"
      },
      "outputs": [],
      "source": [
        "# Plot the average return for one experiment \n",
        "plot(['cartpole_DQN_0'], value='MeanReturn') \n",
        "# Plot the average return for two experiments. If both experiments have the same name, they will be averaged together.\n",
        "plot(['cartpole_PG_0', 'cartpole_PG_1'], value='MeanReturn')\n",
        "# Plot number of timesteps for two experiments, and manually override their name.\n",
        "plot(['cartpole_PG_0', 'cartpole_PG_1'], legend=['exp1', 'exp2'], value='Timesteps')\n",
        "# Plot the average return as a function of the number of episodes\n",
        "plot(['pendulum_PG_0', 'pendulum_DDPG_0'], legend=['pg', 'ddpg'], value='MeanReturn', x='Episodes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QA2F-sd0rOg"
      },
      "source": [
        "# Utilities (run only once)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaZMOlVxM9Tq"
      },
      "source": [
        "Agent initializers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk-nQzCsM9gx"
      },
      "outputs": [],
      "source": [
        "def pg_agent(env, net_params, alg_params):\n",
        "  ob_dim = env.observation_dim\n",
        "  ac_dim = env.action_dim\n",
        "  if env.is_discrete:\n",
        "      policy = Policy(ob_dim, ac_dim, 'softmax', lr=net_params['policy_lr'], hidden=net_params['policy_hidden'])\n",
        "  else:\n",
        "      policy = Policy(ob_dim, ac_dim, 'gaussian', env.max_action, env.min_action, lr=net_params['policy_lr'], hidden=net_params['policy_hidden'])\n",
        "  if net_params['nn_baseline']:\n",
        "      value_fun = ValueFunction(ob_dim, lr=net_params['value_lr'], hidden=net_params['value_hidden'])\n",
        "  else:\n",
        "      value_fun = None\n",
        "\n",
        "  return PG(env, device, policy, value_fun, **alg_params)\n",
        "\n",
        "\n",
        "def dqn_agent(env, net_params, alg_params, exploration):\n",
        "  if not env.is_discrete:\n",
        "      raise RuntimeError('DQN only works for discrete environments')\n",
        "  if alg_params['alternative_car_reward'] and env._short_name != 'car':\n",
        "      raise RuntimeError('You should not use the alternative car reward outside of the car environment')\n",
        "\n",
        "  ob_dim = env.observation_dim\n",
        "  ac_dim = env.action_dim\n",
        "  q_fun = QFunction(ob_dim, ac_dim, discrete=True, lr=net_params['q_lr'], hidden=net_params['q_hidden'], target=True)\n",
        "\n",
        "  return DQN(env, device, q_fun, exploration=exploration, **alg_params)\n",
        "\n",
        "\n",
        "def ddpg_agent(env, net_params, alg_params, exploration):\n",
        "  if env.is_discrete:\n",
        "      raise RuntimeError('DDPG only works for continuous environments')\n",
        "\n",
        "  ob_dim = env.observation_dim\n",
        "  ac_dim = env.action_dim\n",
        "  policy = Policy(ob_dim, ac_dim, 'deterministic', env.max_action, env.min_action,\n",
        "      lr=net_params['policy_lr'], hidden=net_params['policy_hidden'], target=True)\n",
        "  q_fun = QFunction(ob_dim, ac_dim, discrete=False,\n",
        "      lr=net_params['q_lr'], hidden=net_params['q_hidden'], target=True)\n",
        "\n",
        "  return DDPG(env, device, policy, q_fun, exploration_noise=exploration, **alg_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE-_BazkM2HV"
      },
      "source": [
        "Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJqFtMUytltt"
      },
      "outputs": [],
      "source": [
        "def train(agent, max_frames, max_iterations, EVAL_INTERVAL, LOG_INTERVAL, logdir, writer=None):\n",
        "  num_eval_episodes = 2\n",
        "  itr = 1\n",
        "  while agent._frame <= max_frames and itr <= max_iterations:\n",
        "\n",
        "      # collect rollouts\n",
        "      agent.collect_rollouts(itr)\n",
        "          \n",
        "      # update agent\n",
        "      agent.update()\n",
        "\n",
        "      # log diagnostics\n",
        "      if itr % LOG_INTERVAL == 0:\n",
        "          print(\"********** Iteration %i ************\"%itr)\n",
        "          logz.log_tabular(\"Iteration\", itr)\n",
        "          stats = agent.log_progress()\n",
        "          for x in stats.keys():\n",
        "              if writer is not None:\n",
        "                  writer.add_scalar(x, stats[x], itr)\n",
        "              logz.log_tabular(x, stats[x])\n",
        "          logz.dump_tabular()\n",
        "\n",
        "      # eval agent\n",
        "      if itr % EVAL_INTERVAL == 0:\n",
        "          agent.eval_env.setup_recording(os.path.join(logdir, 'eval', 'video_' + str(itr)))\n",
        "          episode_returns, episode_lengths = agent.eval(num_eval_episodes, render=True)\n",
        "          print(\"---------------------------------------\")\n",
        "          print(\"Evaluation over %d episodes: %f\" % (num_eval_episodes, np.mean(episode_returns)))\n",
        "          print(\"---------------------------------------\")\n",
        "          if writer is not None:\n",
        "              writer.add_scalar('EvalMeanReturn', np.mean(episode_returns), itr)\n",
        "          time.sleep(1)\n",
        "          show_videos(os.path.join(logdir, 'eval'), 'video_' + str(itr))\n",
        "\n",
        "      itr += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86GBxcTfNEue"
      },
      "source": [
        "Miscellaneous:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk-rG-AgIgR8"
      },
      "outputs": [],
      "source": [
        "def normalize(values, mean=0., std=1.):\n",
        "    values = (values - values.mean()) / (values.std() + 1e-8)\n",
        "    return mean + (std + 1e-8) * values\n",
        "\n",
        "def make_next_dir(dir_):\n",
        "    dir = dir_ + str(0)\n",
        "    i = 1\n",
        "    while os.path.exists(dir):\n",
        "        dir = dir_ + str(i)\n",
        "        i += 1 \n",
        "    os.mkdir(dir)\n",
        "    os.mkdir(os.path.join(dir, 'eval'))\n",
        "    return dir\n",
        "\n",
        "def setup_dir(dir, params):\n",
        "    logdir = make_next_dir(dir)\n",
        "    logz.configure_output_dir(logdir)\n",
        "    logz.save_params(params)\n",
        "    return logdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74isQdSoz4-s"
      },
      "source": [
        "# 2 Policy gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV_5fypGdvEV"
      },
      "source": [
        "### REINFORCE implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vE6IiQ3d0Fr5"
      },
      "outputs": [],
      "source": [
        "class PG(OnPolicyAgent):\n",
        "    def __init__(self, env, device, policy, value_fun=None, batch_size=64, discount_factor=0.99, normalize_advantages=True, reward_to_go=True, max_path_frames=50\n",
        "        ):\n",
        "        super(PG, self).__init__(env, device, batch_size, max_path_frames)\n",
        "        self.gamma = discount_factor\n",
        "        self.normalize_advantages = normalize_advantages\n",
        "        self.reward_to_go = reward_to_go\n",
        "\n",
        "        # policy network\n",
        "        self.policy = policy\n",
        "        self.policy.to_(self.device)\n",
        "\n",
        "        # value function network\n",
        "        self.value_fun = value_fun\n",
        "        self.nn_baseline = False\n",
        "        if value_fun is not None:\n",
        "            self.nn_baseline = True\n",
        "            self.value_fun.to_(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, ob, eval=False):\n",
        "        ob = self.env.process_state(ob)\n",
        "        if eval:\n",
        "            ac = self.policy.forward(ob, eval=True)\n",
        "        else:\n",
        "            pi = self.policy.net(ob)\n",
        "            ac = pi.sample()\n",
        "        return self.env.process_action(ac)\n",
        "\n",
        "    def update(self):\n",
        "        \"\"\"\n",
        "        :var states: tensor of observations of shape (N, ob_dim)\n",
        "        :var actions: tensor of actions of shape (N, ac_dim) or shape (N) if discrete\n",
        "        :var advs: tensor of advantages of shape (N)\n",
        "        \"\"\"\n",
        "        states = self.buffer.observations()\n",
        "        actions = self.buffer.actions()\n",
        "\n",
        "        if self.nn_baseline:\n",
        "            returns = self.buffer.returns_to_go(self.gamma, add_last_values=True, update_last_values_with=self.value_fun.net) # shape (N)\n",
        "            values = self.value_fun.net(states).view(-1)    # shape (N)\n",
        "\n",
        "            # update baseline\n",
        "            targets = normalize(returns)\n",
        "            value_loss = F.mse_loss(values, targets, reduction='mean')\n",
        "            self.value_fun.optimize(value_loss)\n",
        "\n",
        "            # compute advantages\n",
        "            values = normalize(values.detach(), returns.mean(), returns.std())\n",
        "            advs = returns - values\n",
        "        else:\n",
        "            if self.reward_to_go:\n",
        "                advs = self.buffer.returns_to_go(self.gamma) # shape (N)\n",
        "            else:\n",
        "                advs = self.buffer.returns(self.gamma) # shape (N)\n",
        "\n",
        "        # normalize advantages\n",
        "        if self.normalize_advantages:\n",
        "            advs = normalize(advs)\n",
        "\n",
        "        # update policy\n",
        "        pi = self.policy.net(states)\n",
        "        logprobs = pi.log_prob(actions)\n",
        "        policy_loss = - (logprobs * advs).mean()\n",
        "        self.policy.optimize(policy_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q_I185sevHC"
      },
      "source": [
        "### Training (run experiments for questions 2.1 and 2.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4aOjwdyhP8I"
      },
      "source": [
        "Select an environment and a set of hyper-parameters. Don't forget to set a unique name for each experiment you do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "koDdhYVZfFTa"
      },
      "outputs": [],
      "source": [
        "# name your experiment\n",
        "exp_name = 'myexp'\n",
        "\n",
        "# choose the environment\n",
        "env_names = ['cartpole', 'pendulum', 'lunar', 'lunar-continuous', 'car', 'car-continuous']\n",
        "env_name = env_names[0]\n",
        "\n",
        "# define the training parameters\n",
        "max_training_frames = np.inf        \n",
        "max_training_iterations = 100       # use 100 for cartpole, lunar-continuous       \n",
        "\n",
        "# define the network parameters for the policy (and value function if 'nn_baseline' is True)\n",
        "net_params = {\n",
        "    'nn_baseline': False,            # question 2.2\n",
        "    'value_lr': 0.005,\n",
        "    'policy_lr': 0.005,\n",
        "    'value_hidden': [64],       \n",
        "    'policy_hidden': [32],          # use [32] for cartpole, [64, 64] for lunar-continuous\n",
        "}\n",
        "\n",
        "# define the algorithm parameters\n",
        "alg_params = {\n",
        "    'batch_size': 1000,             # use between 1000 and 5000 for cartpole, 5000 for lunar-continuous\n",
        "    'discount_factor': 0.99,        \n",
        "    'normalize_advantages': False,     # True should help reduce the variance\n",
        "    'reward_to_go': False,             # True should help reduce the variance\n",
        "    'max_path_frames': 10000,\n",
        "}\n",
        "\n",
        "# set up the logging directory (you don't have to change this)\n",
        "logdir = setup_dir('experiments/' + env_name + '_PG_', {**net_params, **alg_params, **{'exp_name': exp_name}})\n",
        "tbdir = os.path.join(logdir, 'runs')\n",
        "cur_run = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjBxL_8TkiWt"
      },
      "source": [
        "If you want, run this cell to monitor the training in real time. MeanReturn and EvalMeanReturn are the most important stats to monitor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rO7Evu6tPciD"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir  $tbdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVwFxOLElD0a"
      },
      "source": [
        "Launch a training session. You can run it more than once by setting num_runs > 1 or by running the cell several times. Different runs from the same experiment will be averaged together when plotting the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9RfRPlqPe4Z"
      },
      "outputs": [],
      "source": [
        "EVAL_INTERVAL = 20 \n",
        "LOG_INTERVAL = 1\n",
        "num_runs = 1\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    env = GymEnv(env_name, device)\n",
        "    agent = pg_agent(env, net_params, alg_params)\n",
        "    writer = SummaryWriter(os.path.join(tbdir, str(cur_run)))\n",
        "    cur_run += 1\n",
        "    train(agent, max_training_frames, max_training_iterations, EVAL_INTERVAL, LOG_INTERVAL, logdir, writer)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOuj1OZxqLRG"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2YS74x7lc5N"
      },
      "source": [
        "**Question 2.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7l2WG69KqPLR"
      },
      "source": [
        "Plot a graph that compares the performance of two different experiments in the cartpole environment, that differ on one (or more) parameters. Are the results expected? You should be able to reach a maximum score of 200 pretty easily.\n",
        "\n",
        "How is exploration handled in policy gradient?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RutvFNtBGEQY"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pKcmgFwF_lM"
      },
      "source": [
        "*Your answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwi0FKLtQEdI"
      },
      "source": [
        "**Question 2.2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_56JaOfQJUp"
      },
      "source": [
        "Plot a graph that compares the performance of two different experiments in the lunar-continuous environment, one with and one without the baseline. Are the results expected? If you choose the recommended parameters, you should reach rewards of around 100, and one experiment should take around 20 minutes. Do not run the experiment without the baseline yourself, use the results from the folder lunar-continuous_PG_0 instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifGh-O9jGEnR"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_zpWuYMGAMP"
      },
      "source": [
        "*Your answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDIRSO_y0GxY"
      },
      "source": [
        "# 3 Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea0YFArSTJCN"
      },
      "source": [
        "### DQN implementation (your code for question 3.2 goes here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZakEnldBunTq"
      },
      "outputs": [],
      "source": [
        "class DQN(OffPolicyAgent):\n",
        "    def __init__(self, env, device, q_fun, batch_size=64, learning_starts=1000, learning_freq=4, replay_buffer_size=50000, max_path_frames=np.inf,\n",
        "                 discount_factor=0.99, exploration=None, sync_every=3000, double_q=False, alternative_car_reward=False\n",
        "        ):\n",
        "        super(DQN, self).__init__(env, device, batch_size, learning_starts, learning_freq, replay_buffer_size, max_path_frames)\n",
        "        self.double_q = double_q\n",
        "        self.exploration = exploration\n",
        "        self.gamma = discount_factor\n",
        "\n",
        "        # q function network\n",
        "        self.q_fun = q_fun\n",
        "        self.q_fun.to_(self.device)\n",
        "        self.q_fun.sync_target()\n",
        "\n",
        "        # utilities for target updates (sync every x iterations)\n",
        "        self._num_updates = 0\n",
        "        self.sync_every = sync_every\n",
        "\n",
        "        # alternative car reward\n",
        "        self.alternative_car_reward = alternative_car_reward\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, ob, eval=False):\n",
        "        if not eval and (self._frame < self.learning_starts or random.random() < self.exploration.value(self._frame)):\n",
        "            ac = self.env.action_space.sample()\n",
        "        else:\n",
        "            ac = self._exploit(ob, eval)\n",
        "        return ac\n",
        "\n",
        "    def _exploit(self, ob, eval):\n",
        "        ob = self.env.process_state(ob)\n",
        "        q = self.q_fun.forward(ob, eval=eval)\n",
        "        ac = torch.argmax(q, axis=1)\n",
        "        return self.env.process_action(ac)\n",
        "\n",
        "    def update(self):\n",
        "        self._update()\n",
        "        self._num_updates += 1\n",
        "\n",
        "        # periodically update the target network\n",
        "        if self._num_updates % self.sync_every == 0:\n",
        "            self.q_fun.sync_target()\n",
        "\n",
        "    def _update(self):\n",
        "        # sample transitions from buffer\n",
        "        data = self.replay_buffer.sample(self.batch_size)\n",
        "        states = data['states']\t\t\t\t\t              # shape (N, ob_dim)\n",
        "        next_states = data['next_states']\t\t          # shape (N, ob_dim)\n",
        "        actions = data['actions'].long().view(-1, 1)  # shape (N, 1)\n",
        "        rewards = data['rewards']\t\t\t\t              # shape (N, 1)\n",
        "        done_mask = data['done_mask']  \t\t\t          # shape (N, 1)\n",
        "\n",
        "        # modify reward for the car environment\n",
        "        if self.alternative_car_reward:\n",
        "            for i, (state, next_state) in enumerate(zip(states, next_states)):\n",
        "                rewards[i] = 100*((math.sin(3*next_state[0]) * 0.0025 + 0.5 * next_state[1] * next_state[1]) - (math.sin(3*state[0]) * 0.0025 + 0.5 * state[1] * state[1]))\n",
        "\n",
        "        # compute q values\n",
        "        q_values = self.q_fun.net(states).gather(1, actions)\t# shape (N, 1)\n",
        "\n",
        "        # compute targets\n",
        "        with torch.no_grad():\n",
        "            if not self.double_q:\n",
        "                q_targets_next = self.q_fun.target_net(next_states).max(1, keepdim=True)[0]  # shape (N, 1)\n",
        "            else:\n",
        "                ### YOUR CODE STARTS HERE (QUESTION 3.2)\n",
        "                raise NotImplementedError() # compute q_targets_next\n",
        "                ### YOUR CODE ENDS HERE\n",
        "            q_targets = rewards + (1 - done_mask) * self.gamma * q_targets_next\n",
        "\n",
        "        # compute loss and update network\n",
        "        loss = F.smooth_l1_loss(q_values, q_targets)\n",
        "        self.q_fun.optimize(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge0-o_rSTl41"
      },
      "source": [
        "### Training (run experiments for questions 3.1 and 3.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5LbSwW2T03E"
      },
      "source": [
        "Select an environment and a set of hyper-parameters. Don't forget to set a unique name for each experiment you do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqP7q-FYT3Dj"
      },
      "outputs": [],
      "source": [
        "# name your experiment\n",
        "exp_name = 'myexp'\n",
        "\n",
        "# choose the environment\n",
        "env_names = ['cartpole', 'lunar', 'car']\n",
        "env_name = env_names[2]\n",
        "\n",
        "# define the training parameters\n",
        "max_training_frames = 150000        # use 150000 for car, 300000 for lunar\n",
        "max_training_iterations = np.inf        \n",
        "\n",
        "# define the network parameters for the q-function\n",
        "net_params = {\n",
        "    'q_lr': 0.005,\n",
        "    'q_hidden': [64],     # use [64] for car, [64, 64] for lunar\n",
        "}\n",
        "\n",
        "# define the algorithm parameters\n",
        "alg_params = {\n",
        "    'batch_size': 32,         # use 32 for car, lunar \n",
        "    'learning_starts': 1000,  # use 1000 for car, lunar (update the model for the first time after collecting at least x frames)\n",
        "    'learning_freq': 1,       # (collect x frames between each model update)\n",
        "    'discount_factor': 0.95,  # use 0.95 for car, 1.00 for lunar\n",
        "    'sync_every': 3000,       # use 3000 for car, lunar (sync the target network every x iterations)\n",
        "    'double_q': False,        # question 3.2\n",
        "    'max_path_frames': 10000,\n",
        "    'replay_buffer_size': 50000,  # use 50000 for car, lunar\n",
        "    'alternative_car_reward': True, # question 3.1\n",
        "}\n",
        "\n",
        "# exploration strategy (you don't have to change this)\n",
        "time2 = max_training_frames * 0.1\n",
        "exploration = PiecewiseSchedule(\n",
        "    [(0, 1), (time2, 0.02)],   # waypoints: exploration threshold decreases linearly from value1 at time1 to value2 at time2, then stays at outside_value\n",
        "    outside_value=0.02)\n",
        "\n",
        "# set up the logging directory (you don't have to change this)\n",
        "logdir = setup_dir('experiments/' + env_name + '_DQN_', {**net_params, **alg_params, **{'exp_name': exp_name}})\n",
        "tbdir = os.path.join(logdir, 'runs')\n",
        "cur_run = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO2Ra4XYUZln"
      },
      "source": [
        "If you want, run this cell to monitor the training in real time. BestMeanReturn, MeanReturn and EvalMeanReturn are the most important stats to monitor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJxK-S9wOKEw"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir  $tbdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAAcwuJlUbuQ"
      },
      "source": [
        "Launch a training session. You can run it more than once by setting num_runs > 1 or by running the cell several times. Different runs from the same experiment will be averaged together when plotting the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imHce1luOOQo"
      },
      "outputs": [],
      "source": [
        "EVAL_INTERVAL = 20000\n",
        "LOG_INTERVAL = 1000\n",
        "num_runs = 1\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    env = GymEnv(env_name, device)\n",
        "    agent = dqn_agent(env, net_params, alg_params, exploration)\n",
        "    writer = SummaryWriter(os.path.join(tbdir, str(cur_run)))\n",
        "    cur_run += 1\n",
        "    train(agent, max_training_frames, max_training_iterations, EVAL_INTERVAL, LOG_INTERVAL, logdir, writer)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74pQpYjXU42y"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdR4z-TbVUL-"
      },
      "source": [
        "**Question 3.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yonsbEzgVYd1"
      },
      "source": [
        "Plot a graph that compares the performance of two different experiments in the car environment, one with the original reward and one with the alternative reward.  Are the results expected? If you choose the recommended parameters, you should reach decent original rewards when training from the alternative rewards, and one experiment should take around 10 minutes. Do not run the experiment with the original reward yourself, use the results from the folder car_DQN_0 instead. Note that the logs always report the original rewards. \n",
        "\n",
        "Other than changing the reward function, which feels a bit like cheating, we could improve the exploration strategy. What do you think might help with the original problem (you don't have to implement it)?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLS-GUqzzekE"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjcUSSju3NXU"
      },
      "source": [
        "*Your answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDcHrEbaVqQN"
      },
      "source": [
        "**Question 3.2**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do9CgqAXVuAV"
      },
      "source": [
        "First, you should complete the code in the 'DQN implementation' cell. Plot a graph that compares the performance of two different experiments in the lunar environment, one with and one without double DQN. Are the results expected? If you choose the recommended parameters, you should reach rewards of around 150, and one experiment should take around 20 minutes. Do not run the experiment without DDQN yourself, use the results from the folder lunar_DQN_0 instead.\n",
        "\n",
        "(Optional) In the previous section you have seen a policy gradient algorithm, which converges in theory (although usually to local optima). DQN doesn't have such guarantees. Can you imagine why? Aren't we doing gradient descent in both cases?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMp1GyTsE1WC"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHFGpEr0FRjh"
      },
      "source": [
        "*Your answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sIC5-RuwZVB"
      },
      "source": [
        "Plot a graph that compares the performance of DQN against Reinforce in the discrete lunar environment. You can also compare the amount of timesteps or episodes seen by each agent. Which was more sample efficient? Why? Do not run the Reinforce experiment yourself, use the results from the folder lunar_PG_0 instead. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KV61YQvNwjY-"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHDf6URKwl3B"
      },
      "source": [
        "*Your answer*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VW6Ao8pY0MH4"
      },
      "source": [
        "# 4 Actor-Critic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o636XxzYWxO"
      },
      "source": [
        "### DDPG implementation (your code for question 4.1 goes here)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yA-B3ov20Q6o"
      },
      "outputs": [],
      "source": [
        "class DDPG(OffPolicyAgent):\n",
        "    def __init__(self, env, device, policy, q_fun, batch_size=64, learning_starts=1000, learning_freq=4, replay_buffer_size=50000, max_path_frames=np.inf,\n",
        "                 discount_factor=0.99, exploration_noise=0.1, sync_every=5, \n",
        "        ):\n",
        "        super(DDPG, self).__init__(env, device, batch_size, learning_starts, learning_freq, replay_buffer_size, max_path_frames)\n",
        "        self.gamma = discount_factor\n",
        "        self.exploration = exploration_noise\n",
        "\n",
        "        # policy network\n",
        "        self.policy = policy\n",
        "        self.policy.to_(self.device)\n",
        "        self.policy.sync_target()\n",
        "\n",
        "        # q function network\n",
        "        self.q_fun = q_fun\n",
        "        self.q_fun.to_(self.device)\n",
        "        self.q_fun.sync_target()\n",
        "\n",
        "        # utilities for target updates (sync every x iterations)\n",
        "        self._num_updates = 0\n",
        "        self.sync_every = sync_every\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def act(self, ob, eval=False):\n",
        "        if not eval and self._frame < self.learning_starts:\n",
        "            ac = self.env.action_space.sample()\n",
        "        else:\n",
        "            ac = self._exploit(ob, eval)\n",
        "            ac_noise = self.exploration.value(self._frame)\n",
        "            if not eval and ac_noise != 0:\n",
        "                ac = ac + np.random.normal(0, ac_noise, size=self.env.action_dim)\n",
        "                ac = ac.clip(self.env.min_action, self.env.max_action)\n",
        "        return ac\n",
        "\n",
        "    def _exploit(self, ob, eval):\n",
        "        ob = self.env.process_state(ob)\n",
        "        ac = self.policy.forward(ob, eval=eval)\n",
        "        return self.env.process_action(ac)\n",
        "\n",
        "    def update(self):\n",
        "        self._update()\n",
        "        self._num_updates += 1\n",
        "\n",
        "        # periodically update the target network\n",
        "        if self._num_updates % self.sync_every == 0:\n",
        "            self.q_fun.sync_target()\n",
        "            self.policy.sync_target()\n",
        "\n",
        "    def _update(self):\n",
        "        # sample transitions from buffer\n",
        "        data = self.replay_buffer.sample(self.batch_size)\n",
        "        states = data['states']\t\t\t\t\t        # shape (N, ob_dim)\n",
        "        next_states = data['next_states']\t\t    # shape (N, ob_dim)\n",
        "        actions = data['actions']\t              # shape (N, ac_dim)\n",
        "        rewards = data['rewards']\t\t\t\t        # shape (N, 1)\n",
        "        done_mask = data['done_mask']  \t\t\t    # shape (N, 1)\n",
        "\n",
        "        # update the critic\n",
        "        q_values = self.q_fun.net(states, actions)      # shape (N, 1)\n",
        "        ### YOUR CODE STARTS HERE (QUESTION 4.1)\n",
        "        raise NotImplementedError() # compute critic_loss\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        self.q_fun.optimize(critic_loss)\n",
        "\n",
        "        # update the actor\n",
        "        self._update_actor(states)\n",
        "\n",
        "    def _update_actor(self, states):\n",
        "        # temporally freeze q-network \n",
        "        for p in self.q_fun.net.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # compute loss and update network\n",
        "        ### YOUR CODE STARTS HERE (QUESTION 4.1)\n",
        "        raise NotImplementedError() # compute actor_loss\n",
        "        ### YOUR CODE ENDS HERE\n",
        "        self.policy.optimize(actor_loss)\n",
        "\n",
        "        # unfreeze q-network\n",
        "        for p in self.q_fun.net.parameters():\n",
        "            p.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQXO7c8R1jU8"
      },
      "source": [
        "### Training (run experiments for question 4.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPON26pJafWK"
      },
      "source": [
        "Select an environment and a set of hyper-parameters. Don't forget to set a unique name for each experiment you do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5eaWsHdccOa"
      },
      "outputs": [],
      "source": [
        "# name your experiment\n",
        "exp_name = 'myexp'\n",
        "\n",
        "# choose the environment\n",
        "env_names = ['pendulum', 'lunar-continuous', 'car-continuous']\n",
        "env_name = env_names[0]\n",
        "\n",
        "# define the training parameters\n",
        "max_training_frames = 200000\n",
        "max_training_iterations = np.inf        \n",
        "\n",
        "# define the network parameters for the policy and q-function\n",
        "net_params = {\n",
        "    'q_lr': 0.005,\n",
        "    'policy_lr': 0.0005,\n",
        "    'q_hidden': [64, 64],\n",
        "    'policy_hidden': [64, 64],\n",
        "}\n",
        "\n",
        "# define the algorithm parameters\n",
        "alg_params = {\n",
        "    'batch_size': 32,\n",
        "    'learning_starts': 1000,\n",
        "    'learning_freq': 1,\n",
        "    'discount_factor': 0.99,\n",
        "    'sync_every': 3000,\n",
        "    'max_path_frames': 10000,\n",
        "    'replay_buffer_size': 50000,\n",
        "}\n",
        "\n",
        "# exploration noise\n",
        "time2 = max_training_frames * 0.1\n",
        "exploration = PiecewiseSchedule(\n",
        "    [(0, 0.5), (time2, 0.02)],\n",
        "    outside_value=0.02)\n",
        "\n",
        "# set up the logging directory (you don't have to change this)\n",
        "logdir = setup_dir('experiments/' + env_name + '_DDPG_', {**net_params, **alg_params, **{'exp_name': exp_name}})\n",
        "tbdir = os.path.join(logdir, 'runs')\n",
        "cur_run = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlW3RnyJamOq"
      },
      "source": [
        "If you want, run this cell to monitor the training in real time. BestMeanReturn, MeanReturn and EvalMeanReturn are the most important stats to monitor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t19o2J66am9T"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir  $tbdir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YynyhCgNapVs"
      },
      "source": [
        "Launch a training session. You can run it more than once by setting num_runs > 1 or by running the cell several times. Different runs from the same experiment will be averaged together when plotting the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2bdtEDFatOn"
      },
      "outputs": [],
      "source": [
        "EVAL_INTERVAL = 20000\n",
        "LOG_INTERVAL = 1000\n",
        "num_runs = 1\n",
        "\n",
        "for _ in range(num_runs):\n",
        "    env = GymEnv(env_name, device)\n",
        "    agent = ddpg_agent(env, net_params, alg_params, exploration)\n",
        "    writer = SummaryWriter(os.path.join(tbdir, str(cur_run)))\n",
        "    cur_run += 1\n",
        "    train(agent, max_training_frames, max_training_iterations, EVAL_INTERVAL, LOG_INTERVAL, logdir, writer)\n",
        "    env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx3HuXxHY4k1"
      },
      "source": [
        "### Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAYk6OijY8Ft"
      },
      "source": [
        "**Question 4.1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T9wBUY6Y__w"
      },
      "source": [
        "First, you should complete the code in the 'DDPG implementation' cell. If you have time, plot a graph of an experiment. For instance, you can compare DDPG to Reinforce in the lunar-continuous environment or the pendulum environment. Did it reach higher rewards (for comparable training times)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEKSqVnzGG4Q"
      },
      "outputs": [],
      "source": [
        "# PLOT GRAPH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuba1vvcGHqu"
      },
      "source": [
        "*Your answer*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Hh7i2Ubs2VhP",
        "74pQpYjXU42y",
        "Ea0YFArSTJCN",
        "ge0-o_rSTl41",
        "wOuj1OZxqLRG",
        "JV_5fypGdvEV",
        "3Q_I185sevHC",
        "bx3HuXxHY4k1",
        "3o636XxzYWxO",
        "KQXO7c8R1jU8"
      ],
      "name": "tprl-lsml-2022.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
